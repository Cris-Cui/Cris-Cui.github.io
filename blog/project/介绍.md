# 基本自我介绍

面试官您好! 很荣幸有这个机会参与应聘贵公司的C++开发工程师一职，感谢面试官和贵公司给我的这次面试机会，我先做一个自我介绍，我叫崔孝俊，来自哈尔滨理工大学网络工程专业，在校期间我担任班长一职，主要负责我们班级的一些日常管理和协助辅导员做一些班级情况的汇报，在学习上我的专业排名在专业前10%，多次获得校园奖学金和哈尔滨银行奖学金，还获得了三好学生的荣誉称号，我的主修课程是计算机网络，操作系统，数据结构与算法等，在学习了计算机网络后，对网络编程比较感兴趣，基于计网和Linux操作系统，我做了一个**基于协程轻量级TCP Socket服务器**，然后再基于这个服务器做了一个**IM**这样比较经典的**业务项目**。

面试官您看我能跟您展开聊聊我的这两个项目吗?

# 项目介绍

## **轻量级高性能TCP Socket服务器框架**

### 基本概述

首先这个轻量级TCP Socket服务器，我基于面向对象思想，主要做了三个大的板块：	

第一个是服务器框架的**基础模块**

工欲善其事，必先利其器。在正式做服务器前，我主要做了日志模块，配置模块，环境变量模块和一些工具类函数和宏封装，这对后期的debug和配置奠定了一个基础。

第二个是针对服务器做的**性能优化模块**，我称之为高并发模块，包括线程模块，线程安全模块，协程模块，协程调度模块，IO协程调度模块，Hook模块，定时器模块。

第三个是**网络库模块**，包括网络地址Address模块，Socket模块和TCPServer模块。

服务器的网络模型是Reactor+线程池模式，IO处理是使用了非阻塞同步IO和IO多路复用技术。服务器目前具备高性能，可扩展性和易用性等特点。我在后面实现服务端的业务功能时，只需要继承TCPServer就可以写自己的业务代码了。然后我对网络库进行了压力测试，在1核2GB的云服务器上测的我的服务器框架，开启一个线程的QPS为5235.75 ，同一台服务器下测得Nginx的QPS为8300. 由此看来在单核处理器下是该服务器框架的吞吐量确实不如Nginx，在单核处理器下Nginx的Worker线程只有一个，可能是Nginx在底层用的异步非阻塞+大量底层优化。但是当我将测试环境从1核的云服务器转到8核的虚拟机中，这个环境，Nginx是开的8个工作者线程，QPS为17000左右，而我的服务器框架在开启8个线程后，QPS为21000左右，所以在多线程多核心数的环境下，该服务器的性能还是比较优异的。确实因为一个线程同一时刻只能执行一个协程，所以在1核场景下，还多了一层协程任务切换的上下文开销。而协程+多线程或者是线程池就能显著表现出优异性。所以我这个服务器的重点和难点就是协程的实现和调度。

面试官我这个项目的重点，难点，亮点：**协程**

我想从三个方面来讲一讲协程。首先我想先讲讲我对协程的理解；其次我讲一讲我为什么要引入协程；最后我在讲讲我在项目中的具体实现，以及实现过程中的一些注意事项。

1. 都说协程是用户态线程，我的理解是协程是看上去华丽花哨，使用上去也花里胡哨的函数。每个协程在创建的时候会指定一个入口函数，这点可以类比线程。所以协程的本质上我认为是函数和函数运行状态的组合。相较于普通函数一旦被调用从函数入口开始执行，直到作用域结束才能退出，而协程可以根据需要在执行到一半的时候主动放弃CPU使用权，并在需要的时候恢复重新运行。而在这一段时间中，其他的协程可以来使用CPU并执行。这就是我认为的他被称为轻量级线程的原因。总结一点就是，协程可以实现程序控制流的主动让出和恢复。这一点在后续协程调度的时候可以让同步的IO达到异步的性能。
2. 我在引入协程之前是使用Epoll+线程池的模型来处理网络并发事件的，操作系统需要给每个线程分配线程栈大小，默认是8MB的，创建大量的线程会导致比较高的系统开销，且线程上下文切换的开销也会比较大，而协程是用户态的，对于操作系统是不可见的，对于协程上下文的切换是在用户层面进行的，不会发生层级转换，且协程栈大小一般比较小，比如128k左右，它的上下文切换开销会比线程小很多。但是并不是说协程切换开销小我就不用线程了，因为协程对内核不可见，他需要线程或者是进程来对他进行承载，而且重要的是，在单线程内协程并不能并发执行，只能是一个协程结束或者主动放弃CPU后，再执行另一个协程，而线程才是真正能执行并发的，毕竟协程只是一个使用比较华丽花哨的函数，无论设计的再精妙，他也不可能在单线程内同时执行两个函数，所在我在实现的时候是N个线程执行M个协程任务引入多线程来利用多核处理器提高性能，以这样的形式来实现协程调度。 还有一点就是在处理IO密集型任务时，协程可以在阻塞时轻松挂起自己，而不会阻塞整个程序。而线程只能阻塞等待IO完成。
3. 第三点就是我的具体实现了，总体来讲，主要流程就是把任务函数包装成一个协程对象，然后再用协程的方式把这个函数跑起来，协程调度就是创建一批协程对象，再创建一个调度协程，通过调度协程把这些协程对象一个一个消化掉，IO协程调度就是当协程被调度时如果在等待IO就绪，就让这个协程先让出执行权，等对应的IO就绪后再重新恢复这个协程的运行。具体来讲，我是用Linux下的ucontext库实现的非对称协程，使用非对称协程的好处在于他相较于对称协程有代码实现简单的特点，因为在对称协程中，子协程可以跟子协程直接进行切换，也就是说每个协程不仅要要运行自己的协程入口函数的代码，还要负责选出下一个合适的协程进行切换，相当于每个协程都要去充当调度器的角色，这要不仅实现比较麻烦，并且程序控制流也会变得负责难以管理，而非对称协程可以借助专门的调度器来负责调度协程，每个协程只需要运行自己的入口函数，然后结束的时候将运行权交还给调度器，由调度器来选出下一个要执行的协程就可以了。



### 项目亮点

在当前互联网环境下，任何一个爆款软件都需要一个高性能的服务器作为支撑，而服务器的吞吐量就是一个很重要的参数，吞吐量由IO处理时间加上业务处理时间。不同的业务不同对应的处理时间也比较参差，所以IO性能是服务器框架尤为重要的一点，在参考了一些博客之后，我了解到了线程之下还有一种调度方式是协程。使用协程的好处是可以进一步优化多线程切换带来的上下文切换开销。如果每个线程中只做非常轻量的操作，但量非常之大，诚然可以通过使用线程池的方式来避免创建销毁的开销，但线程切换的开销还是非常巨大的，甚至可以接近操作本身的开销，在这种场景下，使用协程是进一步优化的不二之选。

但是协程的缺点就是对于操作系统来说是不可见的，需要应用程序自身去完成调度。而且也无法利用多核CPU，所以需要使用线程池+协程调取来处理epoll注册的调度任务。

我在项目中的协程实现是基于ucontext库封装的协程模块，利用ucontext的swapcontext操作可以很好的实现非对称协程。使用非对称协程是因为对称协程的每个协程在执行完任务后需要协程本身去选择下一个要执行的协程，这样每个协程都要充当调度器的任务，这样的操作方式不够灵活和易于理解，所以相比之下，非对称协程+实现一个协程调度器来完成协程上下文切换是比较易于操作的方式，在每个线程中定义两个线程局部变量，当前线程的主协程，以及当前线程的调度协程。维护这两个变量实现非对称调度。

### 项目难点

在实现协程调度时，考虑了把main函数所在的线程也用来执行调度任务，因为前面说到，使用协程是为了减小线程上下文切换的开销。

那在实现相同调度能力的情况下，线程数量越小，线程切换的开销就越小，效率就越高，所以在实现的时候考虑了将调度器所在线程也用来支持执行调度任务。

如何实现这个功能在当时是比较费解的，首先得区分caller线程和callee线程的一些不同点:

- caller线程的主协程并不是调度协程，因为这是一个非对称协程，如果当前程序的主线程的主协程也参与调度，程序就没办法正常结束，所以caller线程的主协程用来保存main函数栈和寄存器，用于程序正常退出。caller线程的子协程为调度协程，通过往任务队列里面放任务协程的方式实现在子协程中定义新的任务协程，并完成上下文切换
- callee线程的主协程是调度协程。每次子协程yield操作时都与主协程切换；子协程是任务协程。



### 常见问答

#### 项目选题相关的问题：

> Q: 为什么不做WebServer?
>
> A: 

> Q: 你这个跟WebServer有什么区别？
>
> A: 

#### 协程相关的问题:

> Q: 非对称协程和对称协程?
>
> A: 

> Q: 使用协程的时候有什么注意事项?
>
> A: 我在实现协程的时候是用的非对称协程，在使用的时候要避免在子协程中创建协程并切换进子协程的子协程。
>
> ​	因为，我在完成协程上下文切换的时候是使用的两个线程局部变量来保存当前线程的主协程，和当前正在使用CPU的协程。如果在  	子协程中

> Q: IO协程调度器的具体实现?
>
> A: 
>
> 前面的Scheduler协程调度器能够实现基本的FIFO协程调度。但仍存在的问题有:
>
> - 调度线程在任务队列空，idle状态下**忙等待导致CPU占用率高**
> - 调度线程对协程任务的调度是无条件执行的，调度器在启动调度的情况下，一旦任务添加成功就会排队等待调度。**不支持删除调度任务**：如果某个fd不再关注(断开连接), 需要将它从epoll_wait中删除。
> - 只执行从任务队列中取任务并执行，不支持监听socket描述符，并在fd就绪时将对应的回调函数(等效成协程)**添加到调度器中进行调度**：服务器需要处理大量来自客户端连接的fd， 使用IO事件调度可以将开发者从判断socket fd是否可读或可写的工作中解放出来，使得程序员只需要关心socket fd的IO操作

#### 设计模式相关问题:

> Q: 你在哪些地方使用了单例模式? 什么是单例模式? 为什么要用单例模式? 单例模式的使用场景?
>
> A: 项目中使用到了单例模式的地方: 1. 日志模块 2. 文件句柄(只包括socket fd)管理类
>
> 

#### 定时器相关问题:

> Q:  介绍一下项目中的定时器模块。为什么要写个定时器，怎么实现的，作用?
>
> A: 定时事件是服务器上经常要处理的一类事件，比如定时检查一个客户连接的活动状态。有效的组织这些定时事件能够在预期时间被触发且不影响服务器的主要逻辑，是服务器的性能来讲是至关重要的。
>
> Linux提供了三种定时方法:
>
> 1. socket选项的SO_RCVTIMEO 和SO_SNDTIMEO，通过errno来判断超时
> 2. SIGALRM信号，SIGALARM信号触发，通过捕获信号来判断超时
> 3. I/O复用系统调用的超时参数，通过判断返回值为0来判断超时
>
> 
>
> 三种定时器容器的设计实现:
>
> ## 基于升序链表的定时器
>
> 1. 所有定时器以单向/双向链表的形式按超时时间升序的方式串联。每个结点包括（1）超时时间（2）回调函数（3）回调函数可选参数（4）下一个结点的指针
> 2. 程序运行后维护一个周期性触发的tick信号(alarm函数周期性触发SIGALARM信号)，在信号处理函数从头到尾遍历定时器链表，判断定时器是否超时。如果定时器超时，就记录下来该结点，并将它从链表中删除。
> 3. 执行所有超时的定时器的回调函数。
>
> **从执行效率上来看**，插入一个结点的时间复杂度是O(n), 删除定时器的时间复杂度是O(1).
>
> **缺点：**
>
> tick信号的周期对定时器的性能有较大的影响，当tick信号周期较小时，定时器精度高，但CPU负担较高，因为要频繁执行信号处理函数；当tick信号周期较大时，CPU负担小，但定时精度差。
>
> 当定时器数量较多时，链表插入操作开销比较大。
>
> **高性能定时器:**
>
> ## 时间轮
>
> 与升序链表相同，也需要维护一个周期性触发的tick信号，但是不同的是，定时器不再单纯组成单链表结构，而是按照超时时间，通过散列分布到不同的时间轮。然后时间轮的每一个卡槽连接一个单链表，同一个链表上的结点满足前后定时时间相差n*时间轮转一周的时间，通过哈希的思想，每次添加定时器散列到不同的链表上，这样插入时间复杂度基本不受定时器数目的影响。
>
> ![img](https://www.midlane.top/wiki/download/attachments/16417216/image2021-7-17_10-34-35.png?version=1&modificationDate=1626489276226&api=v2&effects=drop-shadow)
>
> **缺点:**和升序链表一样，tick的周期将影响定时器精度和CPU负载，除此外，时间轮上的槽数量N还对定时器的效率有影响，N越大，则散列越均匀，插入效率越高，N越小，则散列越容易冲突，至N等于1时，时间轮将完全退化成升序链表。
>
> ## 时间堆
>
> 前两种数据结构设计依赖于一个周期触发的tick信号，而另一种思想是: 直接将定时器的超时时间当作tick周期，做法就是维护一个最小堆，每次取堆顶元素的超时时间作为tick信号，这样，一旦tick触发，超时时间最小的定时器必然会触发。
>
> 对于时间堆而言，添加一个定时器的时间复杂度是O(lgn), 删除一个定时器的时间复杂度是O(1), 效率还是很高的。

> Q: 你是怎么实现对异常断开的客户端的检测的?
>
> A: 我是在应用层实现的类似于KEEPALIVE的机制，Linux socket有一个选项是KEEPALIVE，提供定期对连接是否活动状态的定期检查机制，但是使用这种方式使得应用程序对连接的管理变得复杂。
>
> （1）**如果是基于升序链表实现的定时器**, 可以利用ALARM信号周期性触发SIGALRM信号，在该信号的处理函数中通过管道通知主循环执行定时器链表上的定时任务——关闭那些非活动连接。

#### Hook相关问题:

> Q: 什么是Hook? 为什么要实现Hook模块? 怎么实现Hook模块?
>
> A: Hook实际上是对系统调用的进一步封装，并且是将其封装成一个与原始系统调用同名的接口。用户在使用这些系统调用时可以像正常调用原始的API那样调用，而无需了解底层是怎么运行的。当然，应用程序在调用这些接口时是先执行我们自定义封装的操作，再执行原始的系统调用API。可以类比重载的思想来理解Hook，子类先完成自己的操作，再调用父类。
>
> 而Hook支持我们自定义封装的操作，就可以帮助我们来完成协程调度，让当前执行该阻塞IO的系统调用主动的放弃CPU使用权，实现控制流的转出。可以说Hook和协程调度是密切相关的，如果没有实现协程调度，hook也就没有了意义，如果没有hook，协程调度也不能实现控制流的主动转出和恢复。可以说，只有实现了Hook模块的协程调度才是真正意义上的实现了控制流的自动转出和恢复。实现了Hook模块，可以使一些不具备异步功能的API，展现出异步的性能，比如说MySQL。
>
> 具体在实现上，**重难点:**hook的重点是在替换API的底层实现的同时完全模拟其原本的行为，因为调用方是不知道hook的细节的，在调用被hook的API时，如果其行为与原本的行为不一致，就会给调用方造成困惑。比如，所有的socket fd在进行IO调度时都会被设置成NONBLOCK模式，如果用户未显式地对fd设置NONBLOCK，那就要处理好fcntl，不要对用户暴露fd已经是NONBLOCK的事实，这点也说明，除了IO相关的函数要进行hook外，对fcntl, setsockopt之类的功能函数也要进行hook，才能保证API的一致性。
>
> 我这里的实现是基于动态链接实现的，通过动态库的全局符号介入功能，用自定义的接口来替换掉同名的系统调用接口。由于系统调用接口基本上是由C标准函数库libc提供的，所以这里要做的事情就是用自定义的动态库来覆盖掉libc中的同名符号。具体的做法是在加载libc动态库之前先加载自己实现的hook动态库，

#### 压力测试相关问题:

> Q: 最大连接数? IO吞吐量?
>
> A: 1. 最大连接数
>



#### C++11相关问题:

> Q: share_ptr
>
> A: 拥有共享对象所有权语义的智能指针。
>
> 每个shared_ptr对象内部指向两块内存空间:
>
> - 指向对象
> - 指向用于引用计数的控制数据
>
> **底层采用引用计数的方式实现的。简单的理解，智能指针在申请堆内存时，会为其配备一个整型值（初始值为1），每当有新对象使用该堆内存时，该整型值+1; 反之，每当使用此堆内存的对象释放时，该整型值-1。当堆空间对应的整型值为0时，即表明不再有对象使用它，该堆空间就会被释放掉。**
>
> ##### shared_ptr注意事项：
>
> 避免循环引用
>
> 多线程环境注意线程安全



> Q: 函数包装器 std::function?
>
> `std::function`是通用多态函数封装器。`std::function`的实例能存储、复制及调用任何可调用目标——— 函数，`lambda`表达式，`bind`表达式或者其他函数对象，还有指向成员函数指针和指向数据成员指针。 **用途:**   可以实现函数回调，即在运行时指定要调用的函数，而不是在编译时固定下来。

#### STL相关问题: 

> Q: STL的**六大组件**：
>
> 1. 容器
>
> 	**序列容器**: array, vector, list, deque
>	
> 	**array:** 顺序存储, 在普通数组的基础上封装了一些函数
>	
> 	**vector:** 动态数组，尾增删的时间复杂度是O(1), 头部或中部插入删除的时间复杂度是O(n)
>	
> 	**底层实现机制:**三个指针分别表示vector容器对象的起始字节位置，最后一个元素的末尾字节，整个vector所占用的内存空间的末尾字节，在动态扩容时，如果当size() == capacity()时再向其添加元素，扩容步骤是完全弃用现在的内存空间，申请一片更大的内存空间(2倍或1.5倍)，拷贝旧空间的所有数据，最后再将旧空间释放掉。
>	
> 	**list：**链式存储，底层是以双向队列实现的。
>	
> 	**deque:** 双端队列，底层是以动态分配分段的连续空间，存储在指针数组中，指针指向的一段连续的内存空间，可以随时增加一段更大的空间。
>	
> 	vector和list的区别从四个方面作答:
>	
> 	1. 内存分配空间不同
> 	2. 增删改查效率
> 	3. 迭代器重载操作符不同
> 	4. 使用场景不同
>	
> 	关联容器: 
>	
> 	排列容器: **map, set**
>	
> 	无序关联容器: 哈希map，哈希set
>
> 2. 容器适配器-- stack, queue, priority_queue(优先级队列)， 底层用堆实现
>
> 3. 算法
>
> 4. 迭代器
>
> 5. 函数对象(仿函数)
> 重载了Operator()的Class 
>
> 6. 分配器





![image-20230808203640109](C:%5CUsers%5C28568%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20230808203640109.png)

![image-20230808203858860](C:%5CUsers%5C28568%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20230808203858860.png)

2. 单位时间服务器的IO读写 吞吐量 QPS

	开启五百个客户端一直发东西
	
	打印回显echo_server

TODO:

1. 跑通TcpServer，完成相关知识点梳理和测试
2. 了解项目中用到的有关C++11， STL相关部分的底层逻辑
3. 梳理数据库相关的知识点
4. protobuf 的相关原理

## 基于Qt实现的IM即时通讯软件

我在完成了前面一个服务器项目之后，就开始动手写这个IM项目。在传统的网络编程互发文本的基础上之外，我丰富了该IM的功能，包括注册登录，修改用户信息，添加好友，文件传输，聊天记录，视频文件播放以及音视频通话。

目前服务器放在公网上运行，可以在广域网内使用该桌面软件进行交互。

### 常见问答:

#### ProtoBuf相关问题:

> 1. 我看你项目中用了ProtoBuf，你为什么要用它，怎么用的，有什么好处?
>
> 	我在项目中直接将ProtoBuf的message作为我自定义的应用层协议，在网络中传输序列化后的字符串，并在对端主机反序列化拿到原始字节序进行相应的处理。
>
> 	最开始我在项目中并不是直接用的ProtoBuf，而是通过简单的自定义二进制协议，它的好处在于网络传输的数据量小，没有冗余的数据，这使得这种简单的自定义二进制协议非常的快。但是呢，我在使用过程中也感受到了它的缺陷:
>
> 	1. 首先是编码的复杂度高，自己定义消息格式导致可扩展性不高，比如要添加修改一个字段，需要修改两端的处理逻辑
> 	2. 需要为每个协议预定义协议大小（也就是每个字段占多大）比如在发送聊天消息时，无论发送的文本是长还是短，单个数据包的大小都是固定的长度，这在实际应用场景下，短消息占更高比例，长文本占低比例的情况下，会浪费很多带宽，传输效率很降低。
>
> 	所以我就考虑更换数据传输格式， 我了解到主要有三种:
>
> 	一种就是我使用的自定义二进制协议，第二种就是文本协议，比如xml，json，第三种就是提供序列化和反序列化的开源协议。
>
> 	我在对比之后选择了提供序列化和反序列化的ProtoBuf, 因为首先，文本协议有key-value 的冗余，浪费网络带宽，但它的好处也是显而易见的，相较于二进制协议的复杂性，文本传输能够在开发过程中调试带来便利。但考虑到IM的高并发应用场景的我选择了压缩性能更好的protobuf
>
> 	1. 一条消息数据，使用protobuf序列化之后的大小是json的10分之一，xml的20分之一，是二进制序列化的10分之一。
> 	2. 使用简单，按照语法定义结构化的消息格式，使用命令就可以自动生成相应的类，提供了序列化/反序列化成文本，字节数组的接口，也提供了消息字段的get和set方法。
> 	3. 最后一个点也是它原生支持C++
>
> 	**ProtoBuf性能好的原因:**
>
> 	ProtoBuf之所以在序列化之后要比Json，xml小很多，在于它独特的编码方式。
>
> 	对于正数采用**Varint 编码**是一种紧凑的表示数字的方法，用一个字节或多个字节表示一个数字，值越小使用的字节数越小，比如一个小的int32类型的数字，在使用varint的话可以用1个byte来表示，当然凡事都有不好的一面，大的数字需要5个byte来表示，但是一般来讲不会所有数字都是大的，大多数情况使用varint能压缩消息大小。
>
> 	varint用最高位来表示该数据的字节范围，如果最高位为1，那么表示后续的字节也是该数字的一部分，如果最高位为0，那么表示该字节为最后一个字节。因此，小于128的数字都可以用一个字节表示。
>
> 	对于负数采用Zigzag编码来避免Varint一定需要5个字节来表示负数，因为负数的补码表示最高为是1
>
> 	Zigzag通过无符号数来表示有符号的正数和负数交替，例如使用ZIgzag算法之后表示0 -1，1  -2 ，2 表示为 0， 1，2，3， 4，5

#### 文件传输相关问题:

> Q: 文件秒传, 如何实现的?
>
> A: 

> Q: 大文件传输，如何实现的?， 如何上传的更快？
>
> A:

> Q: 断点续传，如何实现的？
>
> A:

#### 音视频聊天相关问题:



#### 数据库相关问题：

> Q: 聊天记录是怎么实现和存储的？
>
> A: 



# C++

# Linux

## GDB

## 进程&线程&协程

进程：最小的资源分配单位

线程：最小的执行调度单位

协程：用户级线程，比线程更轻量

资源开销：

进程：由于每个进程都有独⽴的内存空间，创建和销毁进程的开销较⼤。进程间切换需要保存和

恢复整个进程的状态，因此上下⽂切换的开销较⾼。

线程：线程共享相同的内存空间，创建和销毁线程的开销较⼩。线程间切换只需要保存和恢复少

量的线程上下⽂，因此上下⽂切换的开销较⼩。

协程：协程共享线程的内存空间

通信与同步：

进程：由于进程间相互隔离，进程之间的通信需要使⽤⼀些特殊机制，如管道、消息队列、共享

内存等。

线程：由于线程共享相同的内存空间，它们之间可以直接访问共享数据，线程间通信更加⽅便。

安全性：

进程：由于进程间相互隔离，⼀个进程的崩溃不会直接影响其他进程的稳定性。

线程：由于线程共享相同的内存空间，⼀个线程的错误可能会影响整个进程的稳定性。

## 静态库和动态库

静态库：.a .lib文件，在链接阶段，将汇编生成的目标.o文件与静态库.lib文件一起打包生成可执行文件

静态库对函数库的链接是放在编译时期完成，程序在运行时与函数库再无瓜葛，方便移植

但是浪费空间和资源，所有相关的目标文件与牵涉到的函数库都被链接合成一个可执行文件

动态库(.so, .dll)空间浪费和静态库的更新对项目更新部署带来麻烦

不同应用程序调用同一份动态库，在内存中只需要有一份动态库的实现

动态库在程序编译时并不会被链接到目标代码中，而是程序运行时才载入

区别就在上面：

1. 可执行文件大小不一样
2. 占用磁盘大小不一样（静态库中同一个函数的代码就会被复制多份，而动态库只有一份）
3. 扩展性和兼容性不一样（静态库发生改变，需要重新编译）
4. 依赖不一样（静态库的可执行文件不需要依赖其他内容即可运行）
5. 加载速度不一样（静态链接比动态链接加载更快）

如何创建一个动态库

分为编译和链接两步， -fPIC是编译选项，PIC是 Position Independent Code 的缩写，表示要生成位置无关的代码，这是动态库需要的特性； -shared是链接选项，告诉gcc生成动态库而不是可执行文件。

| 选项          | 含义                                              |
| ------------- | ------------------------------------------------- |
| -static       | 链接静态库，禁止使用动态库                        |
| -shared       | 进行动态库编译，链接动态库                        |
| -Ldir         | 在动态库的搜索路径中增加dir目录                   |
| -lname        | 链接静态库(libname.a)或动态库(libname.so)的库文件 |
| -fPIC(或fpic) | 生成使用相对地址无关的目标代码                    |

## IO多路复用

多路 - 网络连接

复用 - 在同一个线程、进程

经典的Reactor设计模式，可以实现单一线程/进程监听多个网络IO事件

有select，poll，epoll三种模型。前者是具有跨平台的优势，后两者不具备跨平台的能力，epoll是Linux特有的，但这点我认为其实还好，因为服务器大多部署在Linux系统下，对跨平台移植的要求并不高。我的服务器框架是采用了epoll模型，epoll直接在内核态维护一颗红黑树作为监听集合，直接在内核态创建，省去了很多拷贝开销。而前两者都是在用户态创建监听集合，然后拷贝到内核空间，轮询每一个要监听的文件描述符所注册的事件是否就绪，然后再将就绪的拷贝回用户态，其实这出现了大量无意义的重复开销，浪费系统的资源，poll在select的基础上只是优化了传入集合和传出集合分离，本质上没有什么区别，而epoll没有多余的拷贝开销和挂载到监听设备的开销，当注册的事件就绪后，内核会以回调函数的形式将就绪节点返回到就绪链表，epoll_wait操作只是查看就绪链表是否为空，如果不为空就返回。epoll的好处就在于监听能力得到了很高的提升，没有轮询的最大描述符限制，这点可以通过修改系统配置来改变单进程能打开的最大文件描述符，但就处理能力而言，他们三者没有任何区别，所以连接数量不高的时候，使用io多路复用不一定比多线程+阻塞IO好。

区别：

1. 能监听的文件描述符数量上限：（select是位图，跟位图大小有关(在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符。)，poll是基于链表，没有最大连接数的限制。epoll有连接上限，但是很大，对于单进程能打开的最大文件描述符可以通过修改系统配置

2. 拷贝开销：

	对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

3. epoll的优点：

4. *第一点*，epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 `O(logn)`。而 select/poll 内核里没有类似 epoll 红黑树这种保存所有待检测的 socket 的数据结构，所以 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。

	*第二点*， epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过**回调函数**内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。
	**epoll不会随着监听数量增多而效率降低，能同时监听的Socket的数目也非常多，上限为系统定义的单进程能打开的最大文件描述符个数，epoll是解决C10K的利器**

ET和LT

- 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；
- 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；

# Docker

  虚拟机需要模拟硬件，运行整个操作系统，不但体积臃肿内存占用高，程序的性能也会受到影响

Docker与虚拟机概念比较类似，但轻量很多，不会去模拟底层硬件，只会给每个应用提供完全隔离的运行环境，并且不同docker容器间相互不影响

docker 镜像，可以理解为虚拟机的快照，里面包含了要部署的程序和所关联的库，通过镜像可以创建不同的容器，容器就像运行起来的虚拟机

dockerfile就像一个自动化脚本，          

在实际使用中可以使用多个docker容器来共同协作，比如在一个容器运行web应用，另一个容器运行数据库，做到数据和业务逻辑的有效分离。

